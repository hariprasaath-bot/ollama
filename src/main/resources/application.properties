# Application name
spring.application.name=ollama

# Legacy/custom base URL for manual client (kept for reference)
ollama.base-url=http://localhost:11434

# Spring AI Ollama configuration (used by ChatClient auto-config)
# Prefer environment variable OLLAMA_BASE_URL, fall back to legacy property or default
spring.ai.ollama.base-url=${OLLAMA_BASE_URL:${ollama.base-url}}
# Default model (can be overridden per request in our API)
spring.ai.ollama.chat.options.model=${OLLAMA_MODEL:llama3.1}

# HTTP client timeouts for WebClient (ms). Set response-timeout to 0 to disable.
http.client.connect-timeout-ms=${HTTP_CONNECT_TIMEOUT_MS:10000}
http.client.response-timeout-ms=${HTTP_RESPONSE_TIMEOUT_MS:0}
http.client.read-timeout-ms=${HTTP_READ_TIMEOUT_MS:0}
http.client.write-timeout-ms=${HTTP_WRITE_TIMEOUT_MS:0}
# Increase buffer for large responses (in MB)
http.client.max-in-memory-size-mb=${HTTP_MAX_IN_MEMORY_MB:32}

# Embedding model for vector indexing/search (Ollama model name)
ollama.embedding-model=${OLLAMA_EMBEDDING_MODEL:nomic-embed-text}

# Hugging Face embeddings configuration (used by HuggingFaceEmbeddingClient)
# Base URL for feature-extraction pipeline
huggingface.api.base-url=${HUGGING_FACE_BASE_URL:https://router.huggingface.co/hf-inference/models/intfloat/multilingual-e5-large/pipeline/feature-extraction}
# Embedding model repo id
huggingface.api.model=${HUGGING_FACE_MODEL:}
# API token (optional). Prefer environment variable HUGGING_FACE_API_TOKEN; this property is a fallback.
huggingface.api.token=

server.connection-timeout=600000
